{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c41588f2",
   "metadata": {},
   "source": [
    "# Case Study 1: Claim Resubmission Ingestion Pipeline\n",
    "\n",
    "This notebook implements a comprehensive pipeline for processing insurance claim data from multiple Electronic Medical Records (EMR) systems. The solution addresses key data engineering challenges and evaluation criteria:\n",
    "\n",
    "**Core Functionality:**\n",
    "1. Data Ingestion: processing of multiple EMR sources with different formats\n",
    "2. Schema Normalization: data wrangling and mapping skills\n",
    "3. Eligibility Analysis : logic for resubmission eligibility\n",
    "4. Output Generation: JSON output for downstream automation systems\n",
    "\n",
    "**Key Evaluation Criteria Addressed:**\n",
    "- Data Wrangling & Schema Mapping : Handles CSV and JSON formats with different field structures\n",
    "- Data Handling: Graceful processing of malformed, missing, and inconsistent data\n",
    "- Modular Code: Object-oriented design with specialized processing engines\n",
    "- Clear Communication: Comprehensive logging, comments, and structured output\n",
    "- Ambiguity Handling: Mock LLM classifier for uncertain denial reasons\n",
    "\n",
    "**Final Deliverables:**\n",
    "- Working Jupter notebook with all processing steps\n",
    "- resubmission_candidates.json output file\n",
    "- Comprehensive metrics and logging\n",
    "- error handling for edge cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe6fbd3",
   "metadata": {},
   "source": [
    "### 1. Environment Setup and Library Imports\n",
    "\n",
    "First, we will import all necessary libraries for data processing, file handling, and logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "611b32f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for healthcare claims processing\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Optional, Union, Tuple\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from dataclasses import dataclass, asdict\n",
    "from enum import Enum\n",
    "\n",
    "# Configure comprehensive logging for pipeline monitoring\n",
    "# This addresses the \"Communication\" evaluation criteria\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('healthcare_pipeline.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress pandas warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4f0944",
   "metadata": {},
   "source": [
    "### 2. Data Ingestion Functions\n",
    "\n",
    "These functions handle reading data from different EMR sources with robust error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f6a22a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_csv_data(filepath: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Ingest data from CSV format (EMR Alpha system).\n",
    "    Implements robust error handling for malformed data.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the CSV file\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing claim data\n",
    "        \n",
    "    Error Handling:\n",
    "        - File not found scenarios\n",
    "        - Empty or corrupted files\n",
    "        - Encoding issues\n",
    "        - Malformed CSV structure\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        logger.info(f\"Successfully loaded {len(df)} records from {filepath}\")\n",
    "        \n",
    "        # Convert to list of dictionaries for uniform processing\n",
    "        records = df.to_dict('records')\n",
    "        return records\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File not found: {filepath}\")\n",
    "        return []\n",
    "    except pd.errors.EmptyDataError:\n",
    "        logger.error(f\"Empty or malformed CSV file: {filepath}\")\n",
    "        return []\n",
    "    except pd.errors.ParserError as e:\n",
    "        logger.error(f\"CSV parsing error in {filepath}: {str(e)}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error reading CSV file {filepath}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def ingest_json_data(filepath: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Ingest data from JSON format (EMR Beta system).\n",
    "    Handles various JSON structure inconsistencies.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the JSON file\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing claim data\n",
    "        \n",
    "    Error Handling:\n",
    "        - Invalid JSON syntax\n",
    "        - Unexpected JSON structure\n",
    "        - Encoding issues\n",
    "        - File access problems\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Handle case where JSON is not a list\n",
    "        if not isinstance(data, list):\n",
    "            data = [data] if isinstance(data, dict) else []\n",
    "        \n",
    "        logger.info(f\"Successfully loaded {len(data)} records from {filepath}\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File not found: {filepath}\")\n",
    "        return []\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Invalid JSON format in {filepath}: {str(e)}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error reading JSON file {filepath}: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b6a938",
   "metadata": {},
   "source": [
    "### 3. Schema Normalization \n",
    "\n",
    "These functions transform data from different EMR sources into a unified schema, addressing the core evaluation criteria for data wrangling skills. The normalization process handles:\n",
    "\n",
    "- Field Mapping: Different field names across systems (e.g., 'member' vs 'patient_id')\n",
    "- Data Type Conversion: Consistent data types across sources\n",
    "- Date Normalization: Multiple date formats to ISO standard\n",
    "- Null Handling: Various representations of missing data\n",
    "- Error Recovery: Graceful handling of malformed records\n",
    "\n",
    "**Unified Schema Output:**\n",
    "```json\n",
    "{\n",
    "    \"claim_id\": \"string\",\n",
    "    \"patient_id\": \"string or null\", \n",
    "    \"procedure_code\": \"string\",\n",
    "    \"denial_reason\": \"string or null\",\n",
    "    \"status\": \"string (normalized to lowercase)\",\n",
    "    \"submitted_at\": \"ISO datetime string\",\n",
    "    \"source_system\": \"alpha or beta\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a23561f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_alpha_record(record: Dict) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Normalize a record from EMR Alpha (CSV format).\n",
    "    Handles inconsistent data representations and missing values.\n",
    "    \n",
    "    Args:\n",
    "        record: Raw record from alpha source\n",
    "        \n",
    "    Returns:\n",
    "        Normalized record following unified schema or None if invalid\n",
    "        \n",
    "    Data Handling Features:\n",
    "        - Null value standardization (empty strings, NaN, 'None' text)\n",
    "        - Date format conversion (YYYY-MM-DD to ISO)\n",
    "        - Status normalization (lowercase)\n",
    "        - Error recovery for malformed records\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle multiple representations of missing patient_id\n",
    "        patient_id = record.get('patient_id')\n",
    "        if pd.isna(patient_id) or patient_id == '' or patient_id == 'None':\n",
    "            patient_id = None\n",
    "        \n",
    "        # Normalize denial reason handling\n",
    "        denial_reason = record.get('denial_reason')\n",
    "        if pd.isna(denial_reason) or denial_reason == 'None' or denial_reason == '':\n",
    "            denial_reason = None\n",
    "        \n",
    "        # Parse and normalize date with error handling\n",
    "        submitted_at = record.get('submitted_at')\n",
    "        normalized_date = None\n",
    "        if submitted_at and not pd.isna(submitted_at):\n",
    "            try:\n",
    "                # Primary format: YYYY-MM-DD\n",
    "                normalized_date = datetime.strptime(str(submitted_at), \"%Y-%m-%d\").isoformat()\n",
    "            except ValueError:\n",
    "                # Alternative formats for robust handling\n",
    "                for fmt in [\"%m/%d/%Y\", \"%d/%m/%Y\", \"%Y-%m-%d %H:%M:%S\"]:\n",
    "                    try:\n",
    "                        normalized_date = datetime.strptime(str(submitted_at), fmt).isoformat()\n",
    "                        break\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                else:\n",
    "                    logger.warning(f\"Could not parse date: {submitted_at}\")\n",
    "        \n",
    "        # Create normalized record with unified schema\n",
    "        normalized = {\n",
    "            \"claim_id\": record.get('claim_id'),\n",
    "            \"patient_id\": patient_id,\n",
    "            \"procedure_code\": record.get('procedure_code'),\n",
    "            \"denial_reason\": denial_reason,\n",
    "            \"status\": str(record.get('status', '')).lower(),\n",
    "            \"submitted_at\": normalized_date,\n",
    "            \"source_system\": \"alpha\"\n",
    "        }\n",
    "        \n",
    "        return normalized\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error normalizing alpha record {record}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def normalize_beta_record(record: Dict) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Normalize a record from EMR Beta (JSON format).\n",
    "    Maps different field names to unified schema.\n",
    "    \n",
    "    Args:\n",
    "        record: Raw record from beta source\n",
    "        \n",
    "    Returns:\n",
    "        Normalized record following unified schema or None if invalid\n",
    "        \n",
    "    Schema Mapping:\n",
    "        - 'member' -> 'patient_id'\n",
    "        - 'id' -> 'claim_id'\n",
    "        - 'code' -> 'procedure_code'\n",
    "        - 'error_msg' -> 'denial_reason'\n",
    "        - 'date' -> 'submitted_at' (ISO format conversion)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Map beta field 'member' to unified 'patient_id'\n",
    "        patient_id = record.get('member')\n",
    "        if patient_id is None or patient_id == '' or str(patient_id).lower() == 'none':\n",
    "            patient_id = None\n",
    "        \n",
    "        # Map beta field 'error_msg' to unified 'denial_reason'\n",
    "        denial_reason = record.get('error_msg')\n",
    "        if denial_reason is None or denial_reason == '' or str(denial_reason).lower() == 'none':\n",
    "            denial_reason = None\n",
    "        \n",
    "        # Parse ISO datetime format from beta system\n",
    "        submitted_at = record.get('date')\n",
    "        normalized_date = None\n",
    "        if submitted_at and not pd.isna(submitted_at):\n",
    "            try:\n",
    "                # Handle ISO format with potential timezone information\n",
    "                dt = datetime.fromisoformat(str(submitted_at).replace('T00:00:00', ''))\n",
    "                normalized_date = dt.isoformat()\n",
    "            except ValueError as e:\n",
    "                logger.warning(f\"Could not parse beta date format: {submitted_at}\")\n",
    "        \n",
    "        # Create normalized record with unified schema\n",
    "        normalized = {\n",
    "            \"claim_id\": record.get('id'),\n",
    "            \"patient_id\": patient_id,\n",
    "            \"procedure_code\": record.get('code'),\n",
    "            \"denial_reason\": denial_reason,\n",
    "            \"status\": str(record.get('status', '')).lower(),\n",
    "            \"submitted_at\": normalized_date,\n",
    "            \"source_system\": \"beta\"\n",
    "        }\n",
    "        \n",
    "        return normalized\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error normalizing beta record {record}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825fb865",
   "metadata": {},
   "source": [
    "### 4. Denial Reason Classification System\n",
    "\n",
    "**Handling Ambiguity and Incomplete Input**\n",
    "\n",
    "This section implements intelligent classification for denial reasons, addressing the evaluation criteria for \"Ability to handle ambiguity and incomplete input.\" The system uses:\n",
    "\n",
    "- Rule-Based Classification: Clear business rules for known denial reasons\n",
    "- Heuristic Mapping: Intelligent handling of ambiguous cases\n",
    "- Mock Classifier: Simulates advanced AI classification for uncertain scenarios\n",
    "- Fallback Logic: Graceful handling of null/missing denial reasons\n",
    "\n",
    "**Classification Categories:**\n",
    "- Retryable: Claims that can potentially be corrected and resubmitted\n",
    "- Non-Retryable: Claims with fundamental issues that cannot be easily resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb7260f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denial reason classification system configured\n",
      "Mocked classifier implemented for ambiguous cases\n",
      "Explicit retryable rules: 3\n",
      "Explicit non-retryable rules: 2\n",
      "Ambiguous case mappings: 4\n",
      "Classification system ready for processing\n"
     ]
    }
   ],
   "source": [
    "RETRYABLE_REASONS = {\n",
    "    \"missing modifier\",\n",
    "    \"incorrect npi\", \n",
    "    \"prior auth required\"\n",
    "}\n",
    "\n",
    "NON_RETRYABLE_REASONS = {\n",
    "    \"authorization expired\",\n",
    "    \"incorrect provider type\"\n",
    "}\n",
    "\n",
    "AMBIGUOUS_MAPPINGS = {\n",
    "    \"incorrect procedure\": \"retryable\",  \n",
    "    \"form incomplete\": \"retryable\",     \n",
    "    \"not billable\": \"non_retryable\",    \n",
    "    None: \"non_retryable\"               \n",
    "}\n",
    "\n",
    "def classify_denial_reason(denial_reason: Optional[str]) -> str:\n",
    "    \"\"\"\n",
    "    Classify denial reason as retryable or non-retryable.\n",
    "    Implements mocked classifier for ambiguous cases as per requirements.\n",
    "    \n",
    "    This function demonstrates:\n",
    "    - Clear, testable logic for classification\n",
    "    - Handling of ambiguous and incomplete input\n",
    "    - Robust fallback mechanisms\n",
    "    \n",
    "    Args:\n",
    "        denial_reason: The denial reason text (may be None)\n",
    "        \n",
    "    Returns:\n",
    "        Classification result: 'retryable' or 'non_retryable'\n",
    "    \"\"\"\n",
    "    # Handle None/missing denial reasons gracefully\n",
    "    if denial_reason is None:\n",
    "        logger.info(\"No denial reason provided, classifying as non-retryable\")\n",
    "        return \"non_retryable\"\n",
    "    \n",
    "    # Normalize for consistent comparison\n",
    "    normalized_reason = denial_reason.lower().strip()\n",
    "    \n",
    "    # Apply explicit business rules first\n",
    "    if normalized_reason in RETRYABLE_REASONS:\n",
    "        logger.info(f\"'{denial_reason}' matched explicit retryable rule\")\n",
    "        return \"retryable\"\n",
    "    \n",
    "    if normalized_reason in NON_RETRYABLE_REASONS:\n",
    "        logger.info(f\"'{denial_reason}' matched explicit non-retryable rule\")\n",
    "        return \"non_retryable\"\n",
    "    \n",
    "    # Handle ambiguous cases with mocked classifier logic\n",
    "    if normalized_reason in AMBIGUOUS_MAPPINGS:\n",
    "        result = AMBIGUOUS_MAPPINGS[normalized_reason]\n",
    "        logger.info(f\"'{denial_reason}' classified as {result} via ambiguous mapping\")\n",
    "        return result\n",
    "    # This simulates an AI/ML classifier for handling uncertainty\n",
    "    retryable_keywords = ['missing', 'incorrect', 'incomplete', 'required', 'invalid', 'wrong']\n",
    "    non_retryable_keywords = ['expired', 'unauthorized', 'not covered', 'excluded', 'terminated']\n",
    "    \n",
    "    # Check for retryable indicators\n",
    "    for keyword in retryable_keywords:\n",
    "        if keyword in normalized_reason:\n",
    "            logger.info(f\"'{denial_reason}' classified as retryable based on keyword '{keyword}'\")\n",
    "            return \"retryable\"\n",
    "    # Check for non-retryable indicators\n",
    "    for keyword in non_retryable_keywords:\n",
    "        if keyword in normalized_reason:\n",
    "            logger.info(f\"'{denial_reason}' classified as non-retryable based on keyword '{keyword}'\")\n",
    "            return \"non_retryable\"\n",
    "    # Conservative fallback for completely unknown cases\n",
    "    logger.warning(f\"Unknown denial reason '{denial_reason}', defaulting to non-retryable\")\n",
    "    return \"non_retryable\"\n",
    "\n",
    "def generate_recommended_changes(denial_reason: Optional[str]) -> str:\n",
    "    \"\"\"\n",
    "    Generate actionable recommendations for claim resubmission.\n",
    "    \n",
    "    Args:\n",
    "        denial_reason: The denial reason\n",
    "        \n",
    "    Returns:\n",
    "        Specific recommendation string for claim processing teams\n",
    "    \"\"\"\n",
    "    if denial_reason is None:\n",
    "        return \"Review claim details and resubmit with complete information\"\n",
    "    \n",
    "    normalized_reason = denial_reason.lower().strip()\n",
    "    \n",
    "    # Specific recommendations for known issues\n",
    "    recommendations = {\n",
    "        \"missing modifier\": \"Add appropriate CPT modifier codes and resubmit\",\n",
    "        \"incorrect npi\": \"Review NPI number and resubmit\",\n",
    "        \"prior auth required\": \"Obtain prior authorization before resubmission\",\n",
    "        \"incorrect procedure\": \"Verify procedure code accuracy against documentation\",\n",
    "        \"form incomplete\": \"Complete all required fields including diagnosis codes\"\n",
    "    }\n",
    "    \n",
    "    return recommendations.get(normalized_reason, \n",
    "                             f\"Review and correct '{denial_reason}' issue before resubmission\")\n",
    "\n",
    "# Validation and testing\n",
    "print(\"Denial reason classification system configured\")\n",
    "print(\"Mocked classifier implemented for ambiguous cases\")\n",
    "print(f\"Explicit retryable rules: {len(RETRYABLE_REASONS)}\")\n",
    "print(f\"Explicit non-retryable rules: {len(NON_RETRYABLE_REASONS)}\")\n",
    "print(f\"Ambiguous case mappings: {len(AMBIGUOUS_MAPPINGS)}\")\n",
    "print(\"Classification system ready for processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8ae12f",
   "metadata": {},
   "source": [
    "### 5. Resubmission Eligibility Logic\n",
    "\n",
    "This section implements the core business logic for determining claim resubmission eligibility.\n",
    "**Eligibility Criteria:**\n",
    "\n",
    "1. Status Check: Claim must have 'denied' status\n",
    "2. Patient Validation: Patient ID must be present and valid\n",
    "3. Timing Requirements: Minimum 7-day waiting period after submission\n",
    "4. Denial Classification: Reason must be classified as 'retryable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3c54f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resubmission eligibility logic configured\n",
      "Reference date: 2025-07-30\n",
      "Minimum resubmission period: 7 days\n",
      "Clear, testable business rules implemented\n",
      "Ready to evaluate claims for automated resubmission\n"
     ]
    }
   ],
   "source": [
    "# Current business date for calculations (we assume today is 30 july, 2025)\n",
    "REFERENCE_DATE = datetime(2025, 7, 30) \n",
    "# Minimum waiting period per business rules\n",
    "MIN_DAYS_FOR_RESUBMISSION = 7  \n",
    "\n",
    "def is_eligible_for_resubmission(claim: Dict) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Determine if a claim is eligible for automated resubmission.\n",
    "    \n",
    "    This function implements clear, testable business logic with comprehensive\n",
    "    validation and transparent decision making.\n",
    "    \n",
    "    Eligibility Criteria (All must be satisfied):\n",
    "    1. Status must be 'denied'\n",
    "    2. Patient ID must not be null (data integrity requirement)\n",
    "    3. Claim must be submitted more than 7 days ago (business rule)\n",
    "    4. Denial reason must be classified as 'retryable' (intelligent classification)\n",
    "    \n",
    "    Args:\n",
    "        claim: Normalized claim record dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (is_eligible: bool, decision_reason: str)\n",
    "        \n",
    "    Example:\n",
    "        eligible, reason = is_eligible_for_resubmission(claim)\n",
    "        if eligible:\n",
    "            # Process for resubmission\n",
    "        else:\n",
    "            # Log exclusion reason\n",
    "    \"\"\"\n",
    "    # Claims must be explicitly denied to be eligible for resubmission\n",
    "    claim_status = claim.get('status', '').lower()\n",
    "    if claim_status != 'denied':\n",
    "        return False, f\"Status is '{claim_status}', not 'denied'\"\n",
    "    # Patient ID is required for claim processing and tracking\n",
    "    patient_id = claim.get('patient_id')\n",
    "    if patient_id is None or str(patient_id).strip() == '':\n",
    "        return False, \"Patient ID is null or empty\"\n",
    "    # Claims need minimum waiting period before resubmission attempt\n",
    "    submitted_at = claim.get('submitted_at')\n",
    "    if submitted_at is None:\n",
    "        return False, \"Submission date is missing\"\n",
    "    \n",
    "    try:\n",
    "        submission_date = datetime.fromisoformat(submitted_at)\n",
    "        days_since_submission = (REFERENCE_DATE - submission_date).days\n",
    "        \n",
    "        if days_since_submission <= MIN_DAYS_FOR_RESUBMISSION:\n",
    "            return False, f\"Submitted only {days_since_submission} days ago (requires >{MIN_DAYS_FOR_RESUBMISSION} days)\"\n",
    "    except ValueError as e:\n",
    "        logger.error(f\"Error parsing submission date '{submitted_at}': {str(e)}\")\n",
    "        return False, \"Invalid submission date format\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error processing submission date: {str(e)}\")\n",
    "        return False, \"Error processing submission date\"\n",
    "    # Only retryable denial reasons should be resubmitted\n",
    "    denial_reason = claim.get('denial_reason')\n",
    "    classification = classify_denial_reason(denial_reason)\n",
    "    \n",
    "    if classification != 'retryable':\n",
    "        return False, f\"Denial reason '{denial_reason}' classified as {classification}\"\n",
    "    \n",
    "    # All criteria satisfied - eligible for resubmission\n",
    "    return True, f\"Eligible: denied claim with retryable reason, submitted {days_since_submission} days ago\"\n",
    "\n",
    "def create_resubmission_candidate(claim: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Create a resubmission candidate record in the required output format.\n",
    "    \n",
    "    This function generates the final deliverable format as specified:\n",
    "    {\n",
    "        \"claim_id\": \"A124\",\n",
    "        \"resubmission_reason\": \"Incorrect NPI\", \n",
    "        \"source_system\": \"alpha\",\n",
    "        \"recommended_changes\": \"Review NPI number and resubmit\"\n",
    "    }\n",
    "    \n",
    "    Args:\n",
    "        claim: Eligible normalized claim record\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary in the required resubmission candidate format\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"claim_id\": claim['claim_id'],\n",
    "        \"resubmission_reason\": claim['denial_reason'],\n",
    "        \"source_system\": claim['source_system'],\n",
    "        \"recommended_changes\": generate_recommended_changes(claim['denial_reason'])\n",
    "    }\n",
    "print(\"Resubmission eligibility logic configured\")\n",
    "print(f\"Reference date: {REFERENCE_DATE.strftime('%Y-%m-%d')}\")\n",
    "print(f\"Minimum resubmission period: {MIN_DAYS_FOR_RESUBMISSION} days\")\n",
    "print(\"Clear, testable business rules implemented\")\n",
    "print(\"Ready to evaluate claims for automated resubmission\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d69166",
   "metadata": {},
   "source": [
    "### 6. Main Data Processing Pipeline\n",
    "This section implements the main orchestration pipeline:\n",
    "\n",
    "- Modular Design: Integration of all specialized components\n",
    "- Error Handling: Graceful processing of malformed data\n",
    "- Detailed Logging: Complete audit trail for all operations\n",
    "- Metrics Collection: Comprehensive tracking for final deliverables\n",
    "\n",
    "**Pipeline Steps:**\n",
    "1. Data Ingestion: Load data from multiple EMR sources\n",
    "2. Schema Normalization: Transform to unified format\n",
    "3. Eligibility Analysis: Apply business rules for resubmission\n",
    "4. Output Generation: Create final deliverable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d2f2438",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HealthcarePipelineMetrics:\n",
    "    \"\"\"Class to track pipeline execution metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.total_claims_processed = 0\n",
    "        self.claims_by_source = {}\n",
    "        self.flagged_for_resubmission = 0\n",
    "        self.excluded_claims = 0\n",
    "        self.exclusion_reasons = {}\n",
    "        self.processing_errors = 0\n",
    "    \n",
    "    def add_exclusion_reason(self, reason: str):\n",
    "        \"\"\"Add an exclusion reason to metrics.\"\"\"\n",
    "        if reason not in self.exclusion_reasons:\n",
    "            self.exclusion_reasons[reason] = 0\n",
    "        self.exclusion_reasons[reason] += 1\n",
    "\n",
    "def process_healthcare_claims(alpha_file: str, beta_file: str) -> tuple[List[Dict], HealthcarePipelineMetrics]:\n",
    "    \"\"\"\n",
    "    Main processing function to ingest, normalize, and analyze claims.\n",
    "    \n",
    "    Args:\n",
    "        alpha_file: Path to alpha EMR CSV file\n",
    "        beta_file: Path to beta EMR JSON file\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (resubmission_candidates, metrics)\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting healthcare claims processing pipeline\")\n",
    "    metrics = HealthcarePipelineMetrics()\n",
    "    \n",
    "    try:\n",
    "        # step 1: load data from both sources\n",
    "        logger.info(\"Ingesting data from EMR sources...\")\n",
    "        alpha_records = ingest_csv_data(alpha_file)\n",
    "        beta_records = ingest_json_data(beta_file)\n",
    "        \n",
    "        metrics.claims_by_source['alpha'] = len(alpha_records)\n",
    "        metrics.claims_by_source['beta'] = len(beta_records)\n",
    "        \n",
    "        # step 2: Normalize all records\n",
    "        logger.info(\"Normalizing schemas...\")\n",
    "        normalized_claims = []\n",
    "        \n",
    "        # [rocess alpha records (csv files record)\n",
    "        for record in alpha_records:\n",
    "            try:\n",
    "                normalized = normalize_alpha_record(record)\n",
    "                if normalized:\n",
    "                    normalized_claims.append(normalized)\n",
    "                else:\n",
    "                    metrics.processing_errors += 1\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing alpha record: {e}\")\n",
    "                metrics.processing_errors += 1\n",
    "        \n",
    "        # process beta records (json file records)\n",
    "        for record in beta_records:\n",
    "            try:\n",
    "                normalized = normalize_beta_record(record)\n",
    "                if normalized:\n",
    "                    normalized_claims.append(normalized)\n",
    "                else:\n",
    "                    metrics.processing_errors += 1\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing beta record: {e}\")\n",
    "                metrics.processing_errors += 1\n",
    "        \n",
    "        metrics.total_claims_processed = len(normalized_claims)\n",
    "        logger.info(f\"Normalized {len(normalized_claims)} total claims\")\n",
    "        \n",
    "        # Step 3: analyze eligibility for resubmission\n",
    "        logger.info(\"Analyzing resubmission eligibility...\")\n",
    "        resubmission_candidates = []\n",
    "        \n",
    "        for claim in normalized_claims:\n",
    "            try:\n",
    "                is_eligible, reason = is_eligible_for_resubmission(claim)\n",
    "                \n",
    "                if is_eligible:\n",
    "                    candidate = create_resubmission_candidate(claim)\n",
    "                    resubmission_candidates.append(candidate)\n",
    "                    metrics.flagged_for_resubmission += 1\n",
    "                    logger.debug(f\"Claim {claim['claim_id']} eligible: {reason}\")\n",
    "                else:\n",
    "                    metrics.excluded_claims += 1\n",
    "                    metrics.add_exclusion_reason(reason)\n",
    "                    logger.debug(f\"Claim {claim['claim_id']} excluded: {reason}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error analyzing claim {claim.get('claim_id', 'unknown')}: {e}\")\n",
    "                metrics.processing_errors += 1\n",
    "        \n",
    "        logger.info(f\"Pipeline completed successfully!\")\n",
    "        logger.info(f\"Identified {len(resubmission_candidates)} claims for resubmission\")\n",
    "        \n",
    "        return resubmission_candidates, metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline execution failed: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7c2e1c",
   "metadata": {},
   "source": [
    "### 7. Output Generation and Validation\n",
    "\n",
    "Functions to generate final output and validate the structure for downstream systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "982ff566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_resubmission_candidates(candidates: List[Dict]) -> bool:\n",
    "    \"\"\"\n",
    "    Validate the structure of resubmission candidates.\n",
    "    \n",
    "    Args:\n",
    "        candidates: List of resubmission candidate records\n",
    "        \n",
    "    Returns:\n",
    "        True if all candidates are valid, False otherwise\n",
    "    \"\"\"\n",
    "    required_fields = {'claim_id', 'resubmission_reason', 'source_system', 'recommended_changes'}\n",
    "    \n",
    "    for i, candidate in enumerate(candidates):\n",
    "        if not isinstance(candidate, dict):\n",
    "            logger.error(f\"Candidate {i} is not a dictionary\")\n",
    "            return False\n",
    "        \n",
    "        missing_fields = required_fields - set(candidate.keys())\n",
    "        if missing_fields:\n",
    "            logger.error(f\"Candidate {i} missing fields: {missing_fields}\")\n",
    "            return False\n",
    "        \n",
    "        # Validate field types\n",
    "        if not isinstance(candidate['claim_id'], str):\n",
    "            logger.error(f\"Candidate {i} claim_id is not a string\")\n",
    "            return False\n",
    "    \n",
    "    logger.info(f\"All {len(candidates)} candidates passed validation\")\n",
    "    return True\n",
    "\n",
    "def save_resubmission_candidates(candidates: List[Dict], output_file: str = \"resubmission_candidates.json\") -> bool:\n",
    "    \"\"\"\n",
    "    Save resubmission candidates to JSON file with validation.\n",
    "    \n",
    "    Args:\n",
    "        candidates: List of resubmission candidates\n",
    "        output_file: Output filename\n",
    "        \n",
    "    Returns:\n",
    "        True if saved successfully, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate before saving\n",
    "        if not validate_resubmission_candidates(candidates):\n",
    "            logger.error(\"Validation failed, not saving file\")\n",
    "            return False\n",
    "        \n",
    "        # Save to JSON file\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(candidates, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        logger.info(f\"Results saved to {output_file}\")\n",
    "        \n",
    "        # Verify file was created and readable\n",
    "        try:\n",
    "            with open(output_file, 'r') as f:\n",
    "                verification_data = json.load(f)\n",
    "            logger.info(f\"File verification successful: {len(verification_data)} records\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"File verification failed: {e}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving results: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def display_sample_output(candidates: List[Dict], max_samples: int = 3):\n",
    "    \"\"\"\n",
    "    Display sample resubmission candidates for review.\n",
    "    \n",
    "    Args:\n",
    "        candidates: List of resubmission candidates\n",
    "        max_samples: Maximum number of samples to display\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SAMPLE RESUBMISSION CANDIDATES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if not candidates:\n",
    "        print(\"No candidates found for resubmission\")\n",
    "        return\n",
    "    \n",
    "    for i, candidate in enumerate(candidates[:max_samples], 1):\n",
    "        print(f\"\\n{i}. Claim ID: {candidate['claim_id']}\")\n",
    "        print(f\"Denial Reason: {candidate['resubmission_reason']}\")\n",
    "        print(f\"Source System: {candidate['source_system'].upper()}\")\n",
    "        print(f\"Recommended Action: {candidate['recommended_changes']}\")\n",
    "    \n",
    "    if len(candidates) > max_samples:\n",
    "        print(f\"\\n... and {len(candidates) - max_samples} more candidates\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dfd06d",
   "metadata": {},
   "source": [
    "### 8. Logging and Metrics Collection\n",
    "\n",
    "Comprehensive reporting system for pipeline execution metrics and performance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cacb5b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pipeline_report(metrics: HealthcarePipelineMetrics, candidates: List[Dict]):\n",
    "    \"\"\"\n",
    "    Generate comprehensive pipeline execution report.\n",
    "    \n",
    "    Args:\n",
    "        metrics: Pipeline execution metrics\n",
    "        candidates: List of resubmission candidates\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"HEALTHCARE CLAIMS PIPELINE EXECUTION REPORT\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Execution Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Reference Date: {REFERENCE_DATE.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"\\nPROCESSING SUMMARY\")\n",
    "    print(f\"{'─'*40}\")\n",
    "    print(f\"Total claims processed: {metrics.total_claims_processed}\")\n",
    "    print(f\"Processing errors: {metrics.processing_errors}\")\n",
    "    print(f\"Success rate: {((metrics.total_claims_processed - metrics.processing_errors) / max(metrics.total_claims_processed, 1) * 100):.1f}%\")\n",
    "    \n",
    "    print(f\"\\n SOURCE BREAKDOWN\")\n",
    "    print(f\"{'─'*40}\")\n",
    "    total_source_claims = sum(metrics.claims_by_source.values())\n",
    "    for source, count in metrics.claims_by_source.items():\n",
    "        print(f\"EMR {source.upper()}: {count} claims\")\n",
    "    \n",
    "    print(f\"\\nELIGIBILITY ANALYSIS\")\n",
    "    print(f\"{'─'*40}\")\n",
    "    print(f\"Claims flagged for resubmission: {metrics.flagged_for_resubmission}\")\n",
    "    print(f\"Claims excluded: {metrics.excluded_claims}\")\n",
    "    \n",
    "    if metrics.exclusion_reasons:\n",
    "        print(f\"\\n EXCLUSION REASONS\")\n",
    "        print(f\"{'─'*40}\")\n",
    "        for reason, count in sorted(metrics.exclusion_reasons.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"{reason}: {count}\")\n",
    "    \n",
    "    print(f\"\\n BUSINESS IMPACT\")\n",
    "    print(f\"{'─'*40}\")\n",
    "    if metrics.flagged_for_resubmission > 0:\n",
    "        print(f\" {metrics.flagged_for_resubmission} claims ready for automated resubmission\")\n",
    "        print(f\" Potential revenue recovery opportunity identified\")\n",
    "        print(f\" Recommended next steps: Review and process flagged claims\")\n",
    "    else:\n",
    "        print(f\"No claims currently eligible for resubmission\")\n",
    "        print(f\"Recommended: Review exclusion reasons for improvement opportunities\")\n",
    "    \n",
    "    print(f\"\\n QUALITY METRICS\")\n",
    "    print(f\"{'─'*40}\")\n",
    "    if candidates:\n",
    "        source_distribution = {}\n",
    "        for candidate in candidates:\n",
    "            source = candidate['source_system']\n",
    "            source_distribution[source] = source_distribution.get(source, 0) + 1\n",
    "        \n",
    "        print(\"Resubmission candidates by source:\")\n",
    "        for source, count in source_distribution.items():\n",
    "            print(f\"EMR {source.upper()}: {count}\")\n",
    "    \n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "def save_metrics_report(metrics: HealthcarePipelineMetrics, output_file: str = \"pipeline_metrics.json\"):\n",
    "    \"\"\"\n",
    "    Save detailed metrics to JSON file for further analysis.\n",
    "    \n",
    "    Args:\n",
    "        metrics: Pipeline execution metrics\n",
    "        output_file: Output filename for metrics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        metrics_dict = {\n",
    "            \"execution_timestamp\": datetime.now().isoformat(),\n",
    "            \"reference_date\": REFERENCE_DATE.isoformat(),\n",
    "            \"total_claims_processed\": metrics.total_claims_processed,\n",
    "            \"claims_by_source\": metrics.claims_by_source,\n",
    "            \"flagged_for_resubmission\": metrics.flagged_for_resubmission,\n",
    "            \"excluded_claims\": metrics.excluded_claims,\n",
    "            \"exclusion_reasons\": metrics.exclusion_reasons,\n",
    "            \"processing_errors\": metrics.processing_errors,\n",
    "        }\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(metrics_dict, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Metrics saved to {output_file}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving metrics: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b7d84d",
   "metadata": {},
   "source": [
    "### 9. Pipeline Execution\n",
    "\n",
    "Now let's run the complete healthcare claims processing pipeline with our sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "738eade6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-16 16:09:57,380 - INFO - Starting healthcare claims processing pipeline\n",
      "2025-08-16 16:09:57,384 - INFO - Ingesting data from EMR sources...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Healthcare Claims Processing Pipeline\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-16 16:09:57,467 - INFO - Successfully loaded 6 records from emr_alpha.csv\n",
      "2025-08-16 16:09:57,481 - INFO - Successfully loaded 4 records from emr_beta.json\n",
      "2025-08-16 16:09:57,483 - INFO - Normalizing schemas...\n",
      "2025-08-16 16:09:57,485 - INFO - Normalized 10 total claims\n",
      "2025-08-16 16:09:57,487 - INFO - Analyzing resubmission eligibility...\n",
      "2025-08-16 16:09:57,488 - INFO - 'Missing modifier' matched explicit retryable rule\n",
      "2025-08-16 16:09:57,488 - INFO - 'Incorrect NPI' matched explicit retryable rule\n",
      "2025-08-16 16:09:57,490 - INFO - 'Prior auth required' matched explicit retryable rule\n",
      "2025-08-16 16:09:57,491 - INFO - 'Incorrect provider type' matched explicit non-retryable rule\n",
      "2025-08-16 16:09:57,493 - INFO - 'Missing modifier' matched explicit retryable rule\n",
      "2025-08-16 16:09:57,494 - INFO - Pipeline completed successfully!\n",
      "2025-08-16 16:09:57,495 - INFO - Identified 4 claims for resubmission\n",
      "2025-08-16 16:09:57,496 - INFO - All 4 candidates passed validation\n",
      "2025-08-16 16:09:57,499 - INFO - Results saved to resubmission_candidates.json\n",
      "2025-08-16 16:09:57,516 - INFO - File verification successful: 4 records\n",
      "2025-08-16 16:09:57,519 - INFO - Metrics saved to pipeline_metrics.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "HEALTHCARE CLAIMS PIPELINE EXECUTION REPORT\n",
      "======================================================================\n",
      "Execution Date: 2025-08-16 16:09:57\n",
      "Reference Date: 2025-07-30\n",
      "\n",
      "PROCESSING SUMMARY\n",
      "────────────────────────────────────────\n",
      "Total claims processed: 10\n",
      "Processing errors: 0\n",
      "Success rate: 100.0%\n",
      "\n",
      " SOURCE BREAKDOWN\n",
      "────────────────────────────────────────\n",
      "EMR ALPHA: 6 claims\n",
      "EMR BETA: 4 claims\n",
      "\n",
      "ELIGIBILITY ANALYSIS\n",
      "────────────────────────────────────────\n",
      "Claims flagged for resubmission: 4\n",
      "Claims excluded: 6\n",
      "\n",
      " EXCLUSION REASONS\n",
      "────────────────────────────────────────\n",
      "Patient ID is null or empty: 3\n",
      "Status is 'approved', not 'denied': 2\n",
      "Denial reason 'Incorrect provider type' classified as non_retryable: 1\n",
      "\n",
      " BUSINESS IMPACT\n",
      "────────────────────────────────────────\n",
      " 4 claims ready for automated resubmission\n",
      " Potential revenue recovery opportunity identified\n",
      " Recommended next steps: Review and process flagged claims\n",
      "\n",
      " QUALITY METRICS\n",
      "────────────────────────────────────────\n",
      "Resubmission candidates by source:\n",
      "EMR ALPHA: 3\n",
      "EMR BETA: 1\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "SAMPLE RESUBMISSION CANDIDATES\n",
      "============================================================\n",
      "\n",
      "1. Claim ID: A123\n",
      "Denial Reason: Missing modifier\n",
      "Source System: ALPHA\n",
      "Recommended Action: Add appropriate CPT modifier codes and resubmit\n",
      "\n",
      "2. Claim ID: A124\n",
      "Denial Reason: Incorrect NPI\n",
      "Source System: ALPHA\n",
      "Recommended Action: Review NPI number and resubmit\n",
      "\n",
      "3. Claim ID: A127\n",
      "Denial Reason: Prior auth required\n",
      "Source System: ALPHA\n",
      "Recommended Action: Obtain prior authorization before resubmission\n",
      "\n",
      "4. Claim ID: B988\n",
      "Denial Reason: Missing modifier\n",
      "Source System: BETA\n",
      "Recommended Action: Add appropriate CPT modifier codes and resubmit\n",
      "\n",
      "============================================================\n",
      "\n",
      " Pipeline execution completed successfully!\n",
      " Output files generated:\n",
      "resubmission_candidates.json\n",
      "pipeline_metrics.json\n"
     ]
    }
   ],
   "source": [
    "# define file paths for EMR data sources\n",
    "ALPHA_FILE = \"emr_alpha.csv\"\n",
    "BETA_FILE = \"emr_beta.json\"\n",
    "\n",
    "# execute the complete pipeline\n",
    "try:\n",
    "    print(\"Starting Healthcare Claims Processing Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # process claims through the pipeline\n",
    "    resubmission_candidates, pipeline_metrics = process_healthcare_claims(ALPHA_FILE, BETA_FILE)\n",
    "    \n",
    "    # generate and display comprehensive report\n",
    "    generate_pipeline_report(pipeline_metrics, resubmission_candidates)\n",
    "    \n",
    "    # display sample results\n",
    "    display_sample_output(resubmission_candidates, max_samples=5)\n",
    "    \n",
    "    # save results to files\n",
    "    candidates_saved = save_resubmission_candidates(resubmission_candidates)\n",
    "    metrics_saved = save_metrics_report(pipeline_metrics)\n",
    "    \n",
    "    if candidates_saved and metrics_saved:\n",
    "        print(f\"\\n Pipeline execution completed successfully!\")\n",
    "        print(f\" Output files generated:\")\n",
    "        print(f\"resubmission_candidates.json\")\n",
    "        print(f\"pipeline_metrics.json\")\n",
    "    else:\n",
    "        print(f\"\\n  Pipeline completed with file save issues\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n Pipeline execution failed: {str(e)}\")\n",
    "    logger.error(f\"Pipeline failure: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a1005c",
   "metadata": {},
   "source": [
    "### 10. Results Analysis and Verification\n",
    "\n",
    "Let's examine the detailed results and verify our business logic implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd82442f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-16 16:09:57,539 - INFO - Successfully loaded 6 records from emr_alpha.csv\n",
      "2025-08-16 16:09:57,544 - INFO - Successfully loaded 4 records from emr_beta.json\n",
      "2025-08-16 16:09:57,546 - INFO - 'Missing modifier' matched explicit retryable rule\n",
      "2025-08-16 16:09:57,548 - INFO - 'Missing modifier' matched explicit retryable rule\n",
      "2025-08-16 16:09:57,549 - INFO - 'Incorrect NPI' matched explicit retryable rule\n",
      "2025-08-16 16:09:57,549 - INFO - 'Incorrect NPI' matched explicit retryable rule\n",
      "2025-08-16 16:09:57,550 - INFO - 'Authorization expired' matched explicit non-retryable rule\n",
      "2025-08-16 16:09:57,552 - INFO - 'Prior auth required' matched explicit retryable rule\n",
      "2025-08-16 16:09:57,554 - INFO - 'Prior auth required' matched explicit retryable rule\n",
      "2025-08-16 16:09:57,556 - INFO - 'Incorrect NPI' matched explicit retryable rule\n",
      "2025-08-16 16:09:57,558 - INFO - 'Incorrect provider type' matched explicit non-retryable rule\n",
      "2025-08-16 16:09:57,560 - INFO - 'Incorrect provider type' matched explicit non-retryable rule\n",
      "2025-08-16 16:09:57,561 - INFO - 'Missing modifier' matched explicit retryable rule\n",
      "2025-08-16 16:09:57,562 - INFO - 'Missing modifier' matched explicit retryable rule\n",
      "2025-08-16 16:09:57,565 - INFO - 'incorrect procedure' classified as retryable via ambiguous mapping\n",
      "2025-08-16 16:09:57,566 - INFO - 'Missing modifier' matched explicit retryable rule\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DETAILED RESULTS ANALYSIS\n",
      "==================================================\n",
      " Successfully loaded 4 candidates from file\n",
      "\n",
      " CLAIM-BY-CLAIM ANALYSIS\n",
      "----------------------------------------\n",
      "\n",
      "1. Claim A123 (ALPHA) Ok\n",
      "   Status: denied\n",
      "   Patient ID: P001\n",
      "   Denial Reason: Missing modifier\n",
      "   Submitted: 2025-07-01T00:00:00\n",
      "   Days Since Submission: 29\n",
      "   Reason Classification: retryable\n",
      "   Decision: Eligible: denied claim with retryable reason, submitted 29 days ago\n",
      "\n",
      "2. Claim A124 (ALPHA) Ok\n",
      "   Status: denied\n",
      "   Patient ID: P002\n",
      "   Denial Reason: Incorrect NPI\n",
      "   Submitted: 2025-07-10T00:00:00\n",
      "   Days Since Submission: 20\n",
      "   Reason Classification: retryable\n",
      "   Decision: Eligible: denied claim with retryable reason, submitted 20 days ago\n",
      "\n",
      "3. Claim A125 (ALPHA) Not Okay\n",
      "   Status: denied\n",
      "   Patient ID: NULL\n",
      "   Denial Reason: Authorization expired\n",
      "   Submitted: 2025-07-05T00:00:00\n",
      "   Days Since Submission: 25\n",
      "   Reason Classification: non_retryable\n",
      "   Decision: Patient ID is null or empty\n",
      "\n",
      "4. Claim A126 (ALPHA) Not Okay\n",
      "   Status: approved\n",
      "   Patient ID: P003\n",
      "   Denial Reason: None\n",
      "   Submitted: 2025-07-15T00:00:00\n",
      "   Days Since Submission: 15\n",
      "   Decision: Status is 'approved', not 'denied'\n",
      "\n",
      "5. Claim A127 (ALPHA) Ok\n",
      "   Status: denied\n",
      "   Patient ID: P004\n",
      "   Denial Reason: Prior auth required\n",
      "   Submitted: 2025-07-20T00:00:00\n",
      "   Days Since Submission: 10\n",
      "   Reason Classification: retryable\n",
      "   Decision: Eligible: denied claim with retryable reason, submitted 10 days ago\n",
      "\n",
      "6. Claim A127 (ALPHA) Not Okay\n",
      "   Status: denied\n",
      "   Patient ID: NULL\n",
      "   Denial Reason: Incorrect NPI\n",
      "   Submitted: 2025-07-20T00:00:00\n",
      "   Days Since Submission: 10\n",
      "   Reason Classification: retryable\n",
      "   Decision: Patient ID is null or empty\n",
      "\n",
      "7. Claim B987 (BETA) Not Okay\n",
      "   Status: denied\n",
      "   Patient ID: P010\n",
      "   Denial Reason: Incorrect provider type\n",
      "   Submitted: 2025-07-03T00:00:00\n",
      "   Days Since Submission: 27\n",
      "   Reason Classification: non_retryable\n",
      "   Decision: Denial reason 'Incorrect provider type' classified as non_retryable\n",
      "\n",
      "8. Claim B988 (BETA) Ok\n",
      "   Status: denied\n",
      "   Patient ID: P011\n",
      "   Denial Reason: Missing modifier\n",
      "   Submitted: 2025-07-09T00:00:00\n",
      "   Days Since Submission: 21\n",
      "   Reason Classification: retryable\n",
      "   Decision: Eligible: denied claim with retryable reason, submitted 21 days ago\n",
      "\n",
      "9. Claim B989 (BETA) Not Okay\n",
      "   Status: approved\n",
      "   Patient ID: P012\n",
      "   Denial Reason: None\n",
      "   Submitted: 2025-07-10T00:00:00\n",
      "   Days Since Submission: 20\n",
      "   Decision: Status is 'approved', not 'denied'\n",
      "\n",
      "10. Claim B990 (BETA) Not Okay\n",
      "   Status: denied\n",
      "   Patient ID: NULL\n",
      "   Denial Reason: incorrect procedure\n",
      "   Submitted: 2025-07-01T00:00:00\n",
      "   Days Since Submission: 29\n",
      "   Reason Classification: retryable\n",
      "   Decision: Patient ID is null or empty\n",
      "\n",
      " SUMMARY STATISTICS\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-16 16:09:57,567 - INFO - 'Incorrect NPI' matched explicit retryable rule\n",
      "2025-08-16 16:09:57,569 - INFO - 'Prior auth required' matched explicit retryable rule\n",
      "2025-08-16 16:09:57,571 - INFO - 'Incorrect provider type' matched explicit non-retryable rule\n",
      "2025-08-16 16:09:57,572 - INFO - 'Missing modifier' matched explicit retryable rule\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total claims analyzed: 10\n",
      "Eligible for resubmission: 4\n"
     ]
    }
   ],
   "source": [
    "# Let's analyze the results in detail\n",
    "print(\" DETAILED RESULTS ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load the saved results for verification\n",
    "try:\n",
    "    with open(\"resubmission_candidates.json\", 'r') as f:\n",
    "        saved_candidates = json.load(f)\n",
    "    \n",
    "    print(f\" Successfully loaded {len(saved_candidates)} candidates from file\")\n",
    "    \n",
    "    # Analyze each original claim to understand the decision logic\n",
    "    print(f\"\\n CLAIM-BY-CLAIM ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Re-read original data for analysis\n",
    "    alpha_data = ingest_csv_data(ALPHA_FILE)\n",
    "    beta_data = ingest_json_data(BETA_FILE)\n",
    "    \n",
    "    all_claims = []\n",
    "    \n",
    "    # Process alpha claims\n",
    "    for record in alpha_data:\n",
    "        normalized = normalize_alpha_record(record)\n",
    "        if normalized:\n",
    "            all_claims.append(normalized)\n",
    "    \n",
    "    # Process beta claims\n",
    "    for record in beta_data:\n",
    "        normalized = normalize_beta_record(record)\n",
    "        if normalized:\n",
    "            all_claims.append(normalized)\n",
    "    \n",
    "    # Analyze each claim\n",
    "    for i, claim in enumerate(all_claims, 1):\n",
    "        is_eligible, reason = is_eligible_for_resubmission(claim)\n",
    "        status_icon = \"Ok\" if is_eligible else \"Not Okay\"\n",
    "        \n",
    "        print(f\"\\n{i}. Claim {claim['claim_id']} ({claim['source_system'].upper()}) {status_icon}\")\n",
    "        print(f\"   Status: {claim['status']}\")\n",
    "        print(f\"   Patient ID: {claim['patient_id'] or 'NULL'}\")\n",
    "        print(f\"   Denial Reason: {claim['denial_reason'] or 'None'}\")\n",
    "        print(f\"   Submitted: {claim['submitted_at']}\")\n",
    "        \n",
    "        if claim['submitted_at']:\n",
    "            days_ago = (REFERENCE_DATE - datetime.fromisoformat(claim['submitted_at'])).days\n",
    "            print(f\"   Days Since Submission: {days_ago}\")\n",
    "        \n",
    "        if claim['denial_reason']:\n",
    "            classification = classify_denial_reason(claim['denial_reason'])\n",
    "            print(f\"   Reason Classification: {classification}\")\n",
    "        \n",
    "        print(f\"   Decision: {reason}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n SUMMARY STATISTICS\")\n",
    "    print(\"-\" * 30)\n",
    "    eligible_count = sum(1 for claim in all_claims if is_eligible_for_resubmission(claim)[0])\n",
    "    print(f\"Total claims analyzed: {len(all_claims)}\")\n",
    "    print(f\"Eligible for resubmission: {eligible_count}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\" Results file not found. Please run the pipeline first.\")\n",
    "except Exception as e:\n",
    "    print(f\" Error analyzing results: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6ad598-56d9-43b3-b1a9-6ed28925fa4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
